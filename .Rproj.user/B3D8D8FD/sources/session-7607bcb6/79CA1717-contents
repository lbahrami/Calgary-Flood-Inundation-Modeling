---
title: "Forecasting Flood Inundation"
author: "Leila Bahrami"
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: yes
    toc: yes
    theme: flatly
    toc_float: yes
    code_folding: hide
    number_sections: yes
  pdf_document:
    toc: yes
---
According to the World Health Organization, floods are the most frequent natural disaster affecting countries across the globe. Floods have the potential to leave communities devastated not only because of the direct loss of life due to drowning and infrastructure damage, but also because of indirect impacts such as increased transmission of disease, higher risk of injury and hypothermia, disrupted or disabled infrastructure systems, and increased likelihood of causing other natural disasters. In the past ten years, climate change has exacerbated the effects of natural disasters like flooding, drought, sea level rise, and extreme precipitation and their frequency and intensity are expected to continue to rise if left unchecked.

City planners have the utmost responsibility to ensure their cities are prepared to respond to such natural disasters and minimize harm to their most vulnerable communities. The following document outlines a machine learning algorithm that predicts which areas of a city are at the highest risk of flooding disasters. An Emergency Management team can use the resulting model of this algorithm to inform their preparedness tactics to minimize damage and respond to future flooding disasters more efficiently. Ultimately, the algorithm is a helpful tool for individuals in Public Works, Public Health, City Planning, and Community-Based Organizations to proactively plan for resilience.

The model is created using past flood data from the City of Calgary in Canada to measure accuracy and then is applied to the City of Denver in the United States to measure generalizability. This document explains each step of the process such that City Planning departments can input their city’s data and accurately interpret the resulting model. We also wanted to provide transparency into the feature engineering steps of the process, which is why the procedures in ArcGIS and R are described in detail.

# SetUp
The algorithm was created using ArcGIS Pro and R. First, the necessary R libraries are loaded and themes are defined for the maps and plots that will be displayed later in the process.

```{r setup,  message=FALSE, warning=FALSE, results='hide'}
# load libraries
library(caret)
library(pscl)
library(plotROC)
library(pROC)
library(sf)
library(tidyverse)
library(knitr)
library(kableExtra)
library(FNN)
library(scales)
library(jtools)
library(viridis)
library(gridExtra)


# load themes and palettes
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.text.x = element_text(size = 14))
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}
```  
# Calgary Data

## Initial Data

The fishnet was generated in ArcGIS Pro using the Create Fishnet tool in order to generate a grid of the appropriate extent with 200 meter by 200 meter cells as the unit of measurement. This cell size was determined using the overall extent of the city boundary and the size of the resulting dataset. Cell size will vary depending on the city of focus and the computing power of the machine being used.

The inundation raster data was also generated in ArcGIS Pro using the Reclassify tool to indicate pixels within the flooding extent and pixels outside of the flooding extent. Inundated pixels are assigned a value of 1 and not inundated pixels are assigned a value of 0. Zonal Statistics were then used to calculate the maximum level of flooding per fishnet grid cell in order to visualize and identify which grid cells are at the highest risk of flooding.

```{r Load Data, message=FALSE, warning=FALSE, results='hide' }
# CALGARY: Load Calgary fishnet generated in ArcGIS Pro (200x200m cells).
getwd()
setwd("C:/Users/Leila'Laptop/Documents/GitHub/Calgary-Flood-Inundation-Modeling/Data")
Calgary <- st_read("calgary_criteria.shp")
#plot(Calgary, Inundation)
ggplot(data = Calgary) +
  geom_sf(aes(fill = Inundation)) +  # Color by 'Inundation'
  scale_fill_viridis_c() +           # Optional: Use a color scale from viridis
  theme_minimal() +                  # Apply a minimal theme
  labs(title = "Inundation Map", fill = "Inundation Level") +  # Add title and legend label
  mapTheme()   # Adjust legend position

```  

## Significant Features

Next, data for features that contribute to flooding and flood risk is loaded into ArcGIS Pro for calculations.Knowledge about the characteristics of the land such as topography, land cover, and hydrology can indicate how susceptible the area is to floods.  

### Elevation

Elevation generally describes the topography of the land. Areas at lower levels of elevation are at higher risk for floods than areas at higher elevations. We explored calculating the lowest degree of elevation per fishnet grid cell in Calgary, but found that using the mean helped the performance of our model. Mean elevation is stored in the elevation_mean feature variable, measured in meters.

To calculate our elevation_mean feature variable, a Digital Elevation Model (DEM) was loaded into ArcGIS. Zonal Statistics were used to calculate the mean per fishnet grid cell.
```{r Elevation, message=FALSE, warning=FALSE, results='hide' }
#Maps of All Significant Features included in the Model
map.elevation <- ggplot() +
  geom_sf(data = Calgary, aes(fill = elevation), color=NA) +
  scale_fill_viridis()+
  labs(title = "Elevation (Mean)", fill="Distance \n(meters)") + mapTheme()
```  
### Distance to Nearest Stream
Understanding the structure of streams helps inform the intensity of potential flooding. If the water level of the stream exceeds the stream channel, then flooding occurs. Areas that are closer to streams are more vulnerable to floods than areas that are farther away.

The hydrology tools in ArcGIS Pro are used to turn the Calgary DEM into a stream network. First, the Fill tool was used to fill any sinks. The resulting surface is input in the Flow Direction tool to generate a raster showing the direction of flow out of each pixel. Direction is used to calculate Flow Accumulation per pixel - this step is described further in the next feature section. Assuming a drainage threshold of 25 square kilometers, Set Null is used to calculate the number of pixels that represent a drainage of this area or more, assign these cells a value of 1 and all other pixels a value of NODATA. The resulting Calgary stream network is displayed below. Lastly, the Generate Near Table tool is used to calculate the distance of each fishnet grid cell to a stream, stored as the dist_stream feature variable, measured in meters.
```{r water, message=FALSE, warning=FALSE, results='hide' }
map.water <- ggplot() +
  geom_sf(data = Calgary, aes(fill = dist_water), color=NA) +
  scale_fill_viridis()+
  labs(title = "Distance to \n Nearest Stream", fill="Distance \n(meters)") + mapTheme()
```   
### Maximum Flow Accumulation
Flooding occurs when water levels exceed the stream channel. Based on the direction of the streams calculated above, Flow Accumulation tool is used to calculate the accumulated weight of all pixels flowing into each downslope pixel in the raster. Zonal Statistics (maximum) is used to calculate the maximum flow accumulation per each fishnet grid cell. Using the maximum value allows us to evaluate at the highest risk level. In other words, the resulting model will present a “worst-case-scenario” picture enabling planners to be over-prepared rather than under-prepared. 
```{r Flow Accumulation, message=FALSE, warning=FALSE, results='hide' }
map.flowAccu <- ggplot() +
  geom_sf(data = Calgary, aes(fill = Flow_Accum), color=NA) +
  scale_fill_viridis()+
  labs(title = "Flow Accumulation \n (Maximum)", fill="Distance \n(meters)") + mapTheme()
```  
### Distance to Nearest Steep Slope
Steep slopes in the land can contribute to both flow direction and flow accumulation. Additionally, a steeper slope likely accelerates the speed of flow and can intensify in the case of a flood. Thus, it is important to be aware of the location of steep slopes in the topography and their relative distance to past floods.

Slopes can be calculated using the Slope tool in ArcGIS. To distinguish which slopes are steep, Reclassify is used to assign slopes greater than or equal to 10 a value of 1 and all other slopes a value of NODATA. These steep slopes are converted from raster data to polygon features using the Raster to Polygon tool. Then, Near can be used once again to calculate the distance of each fishnet grid cell to a steep slope. 
```{r Slope, message=FALSE, warning=FALSE, results='hide' }
map.steepslopedist <- ggplot() +
  geom_sf(data = Calgary, aes(fill = dist_slop), color=NA) +
  scale_fill_viridis()+
  labs(title = "Distance to \nNearest Steep Slope", fill="Distance \n(meters)") + mapTheme()
```  
### Distance to Nearest Park
Land cover or soil type can be indicative of areas prone to flooding. The more permeable the surface is, the more risky it is for floods. Initially, land cover data for Calgary was used to identify which fishnet grid cells were impervious based on their land cover type; however, this did not help the model perform well. Instead, distance to parks is incorporated into the model. Parks are generally covered in grass which is fairly permeable. Thus, the closer a fishnet grid cell is to a park, the more likely it is to become inundated.

After bringing in the complete parks dataset in Calgary from the city’s open data portal, the Near tool in ArcGIS is used to perform this calculation for the distance to parks feature variable.
```{r Park, message=FALSE, warning=FALSE, results='hide' }
map.distparks <- ggplot() +
  geom_sf(data = Calgary, aes(fill = dist_park), color=NA) +
  scale_fill_viridis()+
  labs(title = "Distance to \nNearest Park", fill="Distance \n(meters)") + mapTheme()
```  

### Tree Density
Higher tree density increases water absorption through root systems and promotes evapotranspiration, which can significantly reduce surface runoff. Trees also stabilize soil, preventing erosion and increasing water infiltration.

```{r TreeDensity, message=FALSE, warning=FALSE, results='hide' }
map.hydrolength <- ggplot() +
  geom_sf(data = Calgary, aes(fill = tree_dens), color=NA) +
  scale_fill_viridis()+
  labs(title = "Tree Density", fill="Distance \n(meters)") + mapTheme()
```  
### Maps of All Significant Features included in the Model

```{r allfeatures, message=FALSE, warning=FALSE, results='hide' }
grid.arrange(map.elevation, map.water,map.flowAccu,map.steepslopedist, map.hydrolength, map.distparks ,  
             ncol=3,
             top = "")
```  

# Plotting Features per Inundation Outcome

These six selected significant features are shown below in the plots to show differences in across fishnet grid cells that flooded and those that did not, according to the classification from the 2013 inundation extent.
```{r Plot, message=FALSE, warning=FALSE, results='hide' }
#Plotting Features per Inundation Outcome
CalgaryPlotVariables <- 
  Calgary %>%
  as.data.frame() %>%
  select(Inundation, elevation, dist_water, tree_dens, 
         Flow_Accum, dist_slop, dist_park,
         flowlength, dev_sum, PopDens) %>%
  gather(key, value, elevation:PopDens) %>%
  mutate(value = ifelse(key == "Inundation", value*1000, value))


# Do again to scale the bars
CalgaryPlotVariables <- 
  CalgaryPlotVariables %>%
  mutate(value = ifelse(key == "Inundation", value*1000, value))

ggplot(CalgaryPlotVariables, aes(as.factor(Inundation), value, fill=as.factor(Inundation))) + 
  geom_bar(stat="identity") + 
  facet_wrap(~key, scales="free") +
  scale_fill_manual(values = c("dodgerblue4", "darkgreen"),
                    labels = c("Not Inundation","Inundation"),
                    name = "") +
  labs(x="Inundation", y="Value")
```  

# Model Development & Testing on Calgary

Now that the features have been prepared, we can begin to build the predictive model. First, we split up our known data set into a training and a test set.  

## Iterative Development Process

A linear regression is performed on the training set to determine the correlation between the set of selected features for the training data and the inundation data for the training data. This process was repeated multiple times to determine which combination of engineered features performed the best. For instance, recall from the Distance to Parks section that an impervious surfaces feature had been engineered but ultimately was not included in the final model. Reviewing the results from multiple regressions helped us determine which combination of features would provide the best results. The results of the final model are displayed and discussed below:
```{r regression, message=FALSE, warning=FALSE, results='hide' }
# set Training vs Testing
set.seed(3456)
trainIndex <- createDataPartition(Calgary$Inundation, p = .70,
                                  list = FALSE,
                                  times = 1)
CalgaryTrain <- Calgary[ trainIndex,]
CalgaryTest  <- Calgary[-trainIndex,]

InundationModel <- glm(Inundation ~ . - Id, 
                    family="binomial"(link="logit"), data = CalgaryTrain %>%
                      as.data.frame() %>%
                      select(-geometry, -basin, -dist_deve, dist_fores, -Residentia, -dist_fores))
```  
## Results  

### Model Summary
The output displays various statistics that indicate the quality of the model. For instance, the p-value listed for each feature describes how significant the feature is to the outcome of whether or not a flood occurs. All of these features are statistically significant as their p-values are <0.05. Additionally, measures like the Pseudo-R^2 and the AIC score describe how closely related our features are to the outcome. This is the point at which features can be removed, added, or adjusted within the model to enhance performance.
```{r summary, message=FALSE, warning=FALSE }
summary(InundationModel)
```  

### Histogram of classProbs

Once the features have been finalized, we can run the regression model on our test data to further examine its accuracy. The results are displayed in the histogram below showing the frequency of each probability level that a flood will occur.
```{r Histogram, message=FALSE, warning=FALSE, results='hide' }
classProbs <- predict(InundationModel, CalgaryTest, type="response")

hist((classProbs), main = paste("Histogram of classProbs"), col = "blue", xlab = "Inundation Probability") + plotTheme()
```  


### Distribution of Probabilities Visualization

The visualization below shows the distribution of outcomes for the model. It is clear that the model is better at predicting 0s (no inundation) than predicting 1s (inundation).
```{r Probability, message=FALSE, warning=FALSE, results='hide' }
#Distribution of Probabilities Visualization
testProbs <- data.frame(obs = as.numeric(CalgaryTest$Inundation),
                        pred = classProbs)


ggplot(testProbs, aes(x = pred, fill=as.factor(obs))) + geom_density() +
  facet_grid(obs ~ .) + xlab("Probability") + geom_vline(xintercept = .53) +
  scale_fill_manual(values = c("darkgreen", "navy"),
                    labels = c("Not Flooded","Flooded"),
                    name="") +
  labs(title = "Distribution of Probabilities") + plotTheme()
```  

### Finding the Optimal Threshold
In order to best optimize this model for use, identifying the threshold that provides the greatest accuracy is important. This is done by running the iterateThresholds function and looking at the results for all thresholds the table of thresholds 0.01 through 0.99. The optimal threshold in this case is 0.53.
```{r Threshold, message=FALSE, warning=FALSE }
# iterateThresholds function
iterateThresholds <- function(data, group) {
  group <- enquo(group)
  x = .01
  all_prediction <- data.frame()
  while (x <= 1) {
    
    this_prediction <-
      testProbs %>%
      mutate(predOutcome = ifelse(pred > x, 1, 0)) %>%
      group_by(!!group) %>%
      dplyr::count(predOutcome, obs) %>%
      dplyr::summarize(sum_TN = sum(n[predOutcome==0 & obs==0]),
                       sum_TP = sum(n[predOutcome==1 & obs==1]),
                       sum_FN = sum(n[predOutcome==0 & obs==1]),
                       sum_FP = sum(n[predOutcome==1 & obs==0]),
                       total=sum(n)) %>%
      mutate(True_Positive = sum_TP / total,
             True_Negative = sum_TN / total,
             False_Negative = sum_FN / total,
             False_Positive = sum_FP / total,
             Accuracy = (sum_TP + sum_TN) / total, Threshold = x)
    
    all_prediction <- rbind(all_prediction, this_prediction)
    x <- x + .01
  }
  return(all_prediction)
}


whichThreshold <- iterateThresholds(testProbs)

allThresholds<-kable(whichThreshold) %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE)%>%
  scroll_box(width = "100%", height = "500px")

allThresholds
```  

### Confusion Matrix

There are four possible scenarios that will result from the use of this model: 1. Model predicts inundation and there is inundation (True Positive) 2. Model predicts no inundation and there is no inundation (True Negative) 3. Model predicts inundation and there is no inundation (False Positive) 4. Model predicts no inundation and there is inundation (False Negative)

The results of each scenario at the optimal threshold for accuracy (0.53) is displayed in the table below (with the confusion matrix of 0 and 1 directly below the table). These results support our above observation that the model predicts well for areas of no inundation than areas of inundation.
```{r matrix, message=FALSE, warning=FALSE }
testProbs$predClass  = ifelse(testProbs$pred > .53 ,1,0)

xtab.regCalgary <- caret::confusionMatrix(reference = as.factor(testProbs$obs),
                                          data = as.factor(testProbs$predClass),
                                          positive = "1")

as.matrix(xtab.regCalgary) %>% kable(caption = "Confusion Matrix") %>% kable_styling("striped", full_width = T, font_size = 14, position = "left")

```  

### Confusion Matrix - Statistics

Further statistics associated with the confusion matrix are displayed in the table below. Of particular note are the sensitivity (rate of true positives) and specificity (rate of true negatives). While the results indicate that the model performs very well for true negatives (a specificity of 98%), it also predicts the true positives fairly well (a sensitivity of ~72%).

Since it is better to be overprepared for a flooding event than underprepared, we concluded that the number of false positive predictions was not a concern, especially given the precarious and unexpected nature of extreme weather events during an era of climate change.
```{r matrix2, message=FALSE, warning=FALSE }
as.matrix(xtab.regCalgary, what="classes") %>% kable(caption = "Confusion Matrix - Statistics") %>% kable_styling(font_size = 14, full_width = T,
                                                                                                                  bootstrap_options = c("striped", "hover"))
```  

### Map of Predictions for Calgary Testing Set

To spatially investigate the distribution of these four predicted outcomes, the map here shows the four confusion matrix results associated with each fishnet grid cell in the testing set. We elected to only show the predicts for the testing set since the amount of observations was already at a large number and patterns are already discernible.

Note how there are few false positives throughout the city, indicating that there are few places for which the model did not accurately predict inundation. The true positives are concentrated around the area where we saw major hydrological features and the stream network.
```{r matrixstat, message=FALSE, warning=FALSE, results='hide' }
test_predictions <- testProbs %>%
  mutate(TN = predClass==0 & obs==0,
         TP = predClass==1 & obs==1,
         FN =  predClass==0 & obs==1,
         FP = predClass==1 & obs==0)


test_predictions <- test_predictions %>%
  mutate(confResult=case_when(TN == TRUE ~ "True_Negative",
                              TP == TRUE ~ "True_Positive",
                              FN == TRUE ~ "False_Negative",
                              FP == TRUE ~ "False_Positive"))


#join with geometry for the map of prediction classes
cal_test_predictions_mapdata <- cbind(CalgaryTest, test_predictions, by= "ID_FISHNET") %>% st_as_sf()


ggplot() +
  geom_sf(data=cal_test_predictions_mapdata, aes(fill=confResult), colour=NA)+
  scale_fill_discrete()+
  mapTheme() +
  labs(title="Map of Prediction Classes on Calgary Test")
```  

### ROC Curve

The Receiver Operating Characteristic (ROC) Curve for the model is shown below. This curve is a helpful goodness of fit indicator, while helping to visualize trade-offs between true positive and false positive metrics at each threshold from 0.01 to 1. A line going “over” the curve indicates a useful fit. The area under the curve (AUC) here is about 0.96, indicating a useful fit. A reasonable AUC is between 0.5 and 1.
```{r ROC Curve, message=FALSE, warning=FALSE, results='hide'}
#ROC Curve
ggplot(testProbs, aes(d = obs, m = pred)) +
  geom_roc(n.cuts = 50, labels = FALSE) +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') + labs(title="ROC Curve", subtitle="AUC ~ 0.96") + plotTheme()
```  
```{r ROC Area, message=FALSE, warning=FALSE, results='hide'}
auc_calgary <- auc(testProbs$obs, testProbs$pred)
print(auc_calgary)
```  

### Cross-Validation

While the ROC indicates a relatively good fit, cross-validation is key to ensure that the model generalizes well across contexts. A k-fold cross validation (CV) is performed here.
```{r Cross Validation, message=FALSE, warning=FALSE }
#convert inundation to factor
Calgary$Inundation <-as.factor(as.character(Calgary$Inundation))

ctrl <- trainControl(method = "cv",
                     number = 100,
                     savePredictions = TRUE)

cvFit_InundationModel <- train(as.factor(Inundation) ~ ., data = Calgary %>%
                                 as.data.frame() %>%
                                 select(-geometry, -basin, -dist_deve, dist_fores),
                               method="glm", family="binomial", trControl=ctrl)

cvFit_InundationModel
```  

### Accuracy and Kappa

To evaluate the algorithm further, the accuracy an kappa metrics shed light on goodness of fit. Accuracy is the percentage of instances classified correctly out of the total. While this does not provide the nuance provided in the confusion matrix (showing the breakdown of accuracy across the four classes), it is useful in defending the model’s success at a high level.

Kappa (also known as Cohen’s Kappa) is normalized accuracy. This is useful here because we have an imbalance in the 0s and 1s for no inundation and inundation, respectively. While the imbalance is expected given that the previous flood extent showed that the majority of Calgary was not inundated, Kappa is useful in its comparison of observed accuracy to expected accuracy, taking into account random chance. It is still a relatively high value with a mean of over 50%. According to best practices in statistics, a kappa value of 0.5 falls within the range of “substantial agreement.”
```{r Accuracy, message=FALSE, warning=FALSE, results='hide'}
dplyr::select(cvFit_InundationModel$resample, -Resample) %>%
  gather(metric, value) %>%
  left_join(gather(cvFit_InundationModel$results[2:4], metric, mean)) %>%
  ggplot(aes(value)) +
  geom_histogram(bins=35, fill = "navy") +
  facet_wrap(~metric) +
  geom_vline(aes(xintercept = mean), colour = "red", linetype = 3, size = 1.5) +
  scale_x_continuous(limits = c(0, 1)) +
  labs(x="Goodness of Fit", y="Count", title="Accuracy and Kappa",
       subtitle = "Across-fold mean represented as dotted lines") +plotTheme()
```  

### Mapping Calgary Predictions

The final predictions for inundation across Calgary are mapped for each fishnet grid cell. We elected to display the predictions on a probabilities scale to achieve a more continuous surface.
```{r Mapping, message=FALSE, warning=FALSE, results='hide'}
allPredictions <-
  predict(cvFit_InundationModel, Calgary, type="prob")[,2]

calgary_fishnet_final_preds <- Calgary %>%
  cbind(Calgary,allPredictions) %>%
  mutate(allPredictions = round(allPredictions * 100)) 

ggplot() +
  geom_sf(data=calgary_fishnet_final_preds, aes(fill=(allPredictions)), colour=NA) +
  scale_fill_viridis(name = "Probability")+
  mapTheme() +
  labs(title="Predictions for Inundation in Calgary")
```
This second map shows the predicted probabilities, but with the 2013 flood extent overlay. This helps to compare the predicted inundation outcomes with the actual data used to train the model in the first place.
```{r Mapping2013, message=FALSE, warning=FALSE, results='hide'}
ggplot() +
  geom_sf(data=calgary_fishnet_final_preds, aes(fill=(allPredictions)), colour=NA) +
  scale_fill_viridis(name = "Probability")+
  geom_sf(data = calgary_fishnet_final_preds %>%
            filter(Inundation=="1"),
          aes(), color="transparent", fill="red", alpha=0.5)+
  mapTheme() +
  labs(title="Predictions for Inundation in Calgary", subtitle = "2013 flood extent in red overlay")
```  

# Conclusion 

The results of this model are crucial to informing planners and allowing a city to be prepared for flood disasters. Successfully leveraging the knowledge and insight gained can save lives, cities, homes, and money. There are many implementation challenges when it comes to appropriately executing emergency preparedness programs such as funding, political and community support, and collaboration across departments. However, we are confident that the story being told through these data visualizations can be an effective tool in convincing relevant decision-makers